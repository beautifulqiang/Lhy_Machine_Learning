{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYlaRwNu7ojq"
      },
      "source": [
        "# **Homework 2-1 Phoneme Classification**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emUd7uS7crTz"
      },
      "source": [
        "## The DARPA TIMIT Acoustic-Phonetic Continuous Speech Corpus (TIMIT)\n",
        "The TIMIT corpus of reading speech has been designed to provide speech data for the acquisition of acoustic-phonetic knowledge and for the development and evaluation of automatic speech recognition systems.\n",
        "\n",
        "This homework is a multiclass classification task, \n",
        "we are going to train a deep neural network classifier to predict the phonemes for each frame from the speech corpus TIMIT.\n",
        "\n",
        "link: https://academictorrents.com/details/34e2b78745138186976cbc27939b1b34d18bd5b3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVUGfWTo7_Oj"
      },
      "source": [
        "## Download Data\n",
        "Download data from google drive, then unzip it.\n",
        "\n",
        "You should have `timit_11/train_11.npy`, `timit_11/train_label_11.npy`, and `timit_11/test_11.npy` after running this block.<br><br>\n",
        "`timit_11/`\n",
        "- `train_11.npy`: training data<br>\n",
        "- `train_label_11.npy`: training label<br>\n",
        "- `test_11.npy`:  testing data<br><br>\n",
        "\n",
        "**notes: if the google drive link is dead, you can download the data directly from Kaggle and upload it to the workspace**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzkiMEcC3Foq",
        "outputId": "4308c64c-6885-4d1c-8eb7-a2d9b8038401"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'gdown' �����ڲ����ⲿ���Ҳ���ǿ����еĳ���\n",
            "���������ļ���\n",
            "unzip:  cannot find either data.zip or data.zip.zip.\n",
            "'ls' �����ڲ����ⲿ���Ҳ���ǿ����еĳ���\n",
            "���������ļ���\n"
          ]
        }
      ],
      "source": [
        "!gdown --id '1HPkcmQmFGu-3OknddKIa5dNDsR05lIQR' --output data.zip\n",
        "!unzip data.zip\n",
        "!ls "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_L_4anls8Drv"
      },
      "source": [
        "## Preparing Data\n",
        "Load the training and testing data from the `.npy` file (NumPy array)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJjLT8em-y9G",
        "outputId": "8edc6bfe-7511-447f-f239-00b96dba6dcf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data ...\n",
            "Size of training data: (1229932, 429)\n",
            "Size of testing data: (451552, 429)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "print('Loading data ...')\n",
        "\n",
        "data_root='./timit_11/'\n",
        "train = np.load(data_root + 'train_11.npy')\n",
        "train_label = np.load(data_root + 'train_label_11.npy')\n",
        "test = np.load(data_root + 'test_11.npy')\n",
        "\n",
        "print('Size of training data: {}'.format(train.shape))\n",
        "print('Size of testing data: {}'.format(test.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "us5XW_x6udZQ"
      },
      "source": [
        "## Create Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "Fjf5EcmJtf4e"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class TIMITDataset(Dataset):\n",
        "    def __init__(self, X, y=None):\n",
        "        # X为numpy数组，而y为标签，没有提供y则默认为None\n",
        "        self.data = torch.from_numpy(X).float()  # 将数据转变为torch内的张量，并且转为float型\n",
        "        if y is not None:\n",
        "            # 若提供y，则将其转换为PyTorch中的长整型张量\n",
        "            y = y.astype(np.int32)\n",
        "            self.label = torch.LongTensor(y)\n",
        "        else:\n",
        "            self.label = None\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # 这个方法是通过索引idx获取数据集中的一个样本\n",
        "        if self.label is not None:\n",
        "            # 标签也存在则返回数据和对应标签\n",
        "            return self.data[idx], self.label[idx]\n",
        "        else:\n",
        "            # 否则只返回数据\n",
        "            return self.data[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otIC6WhGeh9v"
      },
      "source": [
        "Split the labeled data into a training set and a validation set, you can modify the variable `VAL_RATIO` to change the ratio of validation data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYqi_lAuvC59",
        "outputId": "13dabe63-4849-47ee-fe04-57427b9d601c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Size of training set: (983945, 429)\n",
            "Size of validation set: (245987, 429)\n"
          ]
        }
      ],
      "source": [
        "VAL_RATIO = 0.2  # 这里是划分交叉验证集和训练数据集的比例的地方，是个hyperparameter\n",
        "\n",
        "percent = int(train.shape[0] * (1 - VAL_RATIO))\n",
        "train_x, train_y = train[:percent], train_label[:percent]\n",
        "val_x, val_y = train[percent:], train_label[percent:]\n",
        "print('Size of training set: {}'.format(train_x.shape))\n",
        "print('Size of validation set: {}'.format(val_x.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbCfclUIgMTX"
      },
      "source": [
        "Create a data loader from the dataset, feel free to tweak the variable `BATCH_SIZE` here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "RUCbQvqJurYc"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 2048  # 这是设置批大小的地方，这里可以自由调整\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_set = TIMITDataset(train_x, train_y)\n",
        "val_set = TIMITDataset(val_x, val_y)\n",
        "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True) #only shuffle the training data\n",
        "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SY7X0lUgb50"
      },
      "source": [
        "Cleanup the unneeded variables to save memory.<br>\n",
        "\n",
        "**notes: if you need to use these variables later, then you may remove this block or clean up unneeded variables later<br>the data size is quite huge, so be aware of memory usage in colab**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8rzkGraeYeN",
        "outputId": "dc790996-a43c-4a99-90d4-e7928892a899"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "580"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gc\n",
        "\n",
        "del train, train_label, train_x, train_y, val_x, val_y\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRqKNvNZwe3V"
      },
      "source": [
        "## Create Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYr1ng5fh9pA"
      },
      "source": [
        "Define model architecture, you are encouraged to change and experiment with the model architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "lbZrwT6Ny0XL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.layer0 = nn.Linear(429, 2048)\n",
        "        self.bn0 = nn.BatchNorm1d(2048)  # 这里加入了batch normalization的优化尝试，对每层均进行\n",
        "        self.layer1 = nn.Linear(2048, 1024)\n",
        "        self.bn1 = nn.BatchNorm1d(1024)\n",
        "        self.layer2 = nn.Linear(1024, 512)\n",
        "        self.bn2 = nn.BatchNorm1d(512)\n",
        "        self.layer3 = nn.Linear(512, 256)\n",
        "        self.bn3 = nn.BatchNorm1d(256)\n",
        "        self.layer4 = nn.Linear(256, 128)\n",
        "        self.bn4 = nn.BatchNorm1d(128)\n",
        "        self.layer5 = nn.Linear(128, 64)\n",
        "        self.bn5 = nn.BatchNorm1d(64)\n",
        "        self.out = nn.Linear(64, 39) \n",
        "\n",
        "        self.act_fn = nn.ReLU()\n",
        "        self.drop = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer0(x)\n",
        "        x = self.bn0(x)\n",
        "        x = self.act_fn(x)\n",
        "        x = self.drop(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.act_fn(x)\n",
        "        x = self.drop(x)\n",
        "\n",
        "        x = self.layer2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.act_fn(x)\n",
        "        x = self.drop(x)\n",
        "\n",
        "        x = self.layer3(x)\n",
        "        x = self.bn3(x)\n",
        "        x = self.act_fn(x)\n",
        "        x = self.drop(x)\n",
        "\n",
        "        x = self.layer4(x)\n",
        "        x = self.bn4(x)\n",
        "        x = self.act_fn(x)\n",
        "        x = self.drop(x)\n",
        "\n",
        "        x = self.layer5(x)\n",
        "        x = self.bn5(x)\n",
        "        x = self.act_fn(x)\n",
        "        x = self.drop(x)\n",
        "\n",
        "        x = self.out(x)\n",
        "        \n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRYciXZvPbYh"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "y114Vmm3Ja6o"
      },
      "outputs": [],
      "source": [
        "#check device\n",
        "def get_device():\n",
        "  return 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEX-yjHjhGuH"
      },
      "source": [
        "Fix random seeds for reproducibility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "88xPiUnm0tAd"
      },
      "outputs": [],
      "source": [
        "# fix random seed\n",
        "def same_seeds(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)  \n",
        "    np.random.seed(seed)  \n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbBcBXkSp6RA"
      },
      "source": [
        "Feel free to change the training parameters here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "QTp3ZXg1yO9Y"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DEVICE: cuda\n"
          ]
        }
      ],
      "source": [
        "# fix random seed for reproducibility\n",
        "same_seeds(0)\n",
        "\n",
        "# get device \n",
        "device = get_device()\n",
        "print(f'DEVICE: {device}')\n",
        "\n",
        "# training parameters, 这一块可以自己调整更改\n",
        "num_epoch = 100               # number of training epoch\n",
        "learning_rate = 0.001       # learning rate\n",
        "\n",
        "# the path where checkpoint saved\n",
        "model_path = './model.ckpt'\n",
        "\n",
        "# create model, define a loss function, and optimizer\n",
        "model = Classifier().to(device)\n",
        "criterion = nn.CrossEntropyLoss() \n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdMWsBs7zzNs",
        "outputId": "c5ed561e-610d-4a35-d936-fd97adf342a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[001/100] Train Acc: 0.402867 Loss: 2.151267 | Val Acc: 0.544919 loss: 1.504248\n",
            "saving model with acc 0.545\n",
            "[002/100] Train Acc: 0.505922 Loss: 1.721443 | Val Acc: 0.602072 loss: 1.330331\n",
            "saving model with acc 0.602\n",
            "[003/100] Train Acc: 0.542908 Loss: 1.609000 | Val Acc: 0.636615 loss: 1.236910\n",
            "saving model with acc 0.637\n",
            "[004/100] Train Acc: 0.566218 Loss: 1.540344 | Val Acc: 0.655994 loss: 1.172505\n",
            "saving model with acc 0.656\n",
            "[005/100] Train Acc: 0.583814 Loss: 1.486284 | Val Acc: 0.671161 loss: 1.131601\n",
            "saving model with acc 0.671\n",
            "[006/100] Train Acc: 0.598720 Loss: 1.442010 | Val Acc: 0.682443 loss: 1.095370\n",
            "saving model with acc 0.682\n",
            "[007/100] Train Acc: 0.608711 Loss: 1.408498 | Val Acc: 0.690728 loss: 1.068428\n",
            "saving model with acc 0.691\n",
            "[008/100] Train Acc: 0.617685 Loss: 1.380134 | Val Acc: 0.695390 loss: 1.050893\n",
            "saving model with acc 0.695\n",
            "[009/100] Train Acc: 0.625185 Loss: 1.355061 | Val Acc: 0.701053 loss: 1.028821\n",
            "saving model with acc 0.701\n",
            "[010/100] Train Acc: 0.631098 Loss: 1.333558 | Val Acc: 0.704419 loss: 1.017567\n",
            "saving model with acc 0.704\n",
            "[011/100] Train Acc: 0.636857 Loss: 1.314147 | Val Acc: 0.709485 loss: 1.000835\n",
            "saving model with acc 0.709\n",
            "[012/100] Train Acc: 0.641665 Loss: 1.297389 | Val Acc: 0.711416 loss: 0.993685\n",
            "saving model with acc 0.711\n",
            "[013/100] Train Acc: 0.645584 Loss: 1.282342 | Val Acc: 0.714578 loss: 0.982874\n",
            "saving model with acc 0.715\n",
            "[014/100] Train Acc: 0.649189 Loss: 1.269061 | Val Acc: 0.715704 loss: 0.975265\n",
            "saving model with acc 0.716\n",
            "[015/100] Train Acc: 0.653239 Loss: 1.257646 | Val Acc: 0.718538 loss: 0.968003\n",
            "saving model with acc 0.719\n",
            "[016/100] Train Acc: 0.656152 Loss: 1.246180 | Val Acc: 0.721644 loss: 0.958627\n",
            "saving model with acc 0.722\n",
            "[017/100] Train Acc: 0.659002 Loss: 1.234611 | Val Acc: 0.720810 loss: 0.953435\n",
            "[018/100] Train Acc: 0.661610 Loss: 1.225496 | Val Acc: 0.723327 loss: 0.947252\n",
            "saving model with acc 0.723\n",
            "[019/100] Train Acc: 0.663774 Loss: 1.215426 | Val Acc: 0.723998 loss: 0.949296\n",
            "saving model with acc 0.724\n",
            "[020/100] Train Acc: 0.666572 Loss: 1.208895 | Val Acc: 0.725335 loss: 0.941391\n",
            "saving model with acc 0.725\n",
            "[021/100] Train Acc: 0.668311 Loss: 1.199665 | Val Acc: 0.726912 loss: 0.939561\n",
            "saving model with acc 0.727\n",
            "[022/100] Train Acc: 0.669307 Loss: 1.195053 | Val Acc: 0.728421 loss: 0.934296\n",
            "saving model with acc 0.728\n",
            "[023/100] Train Acc: 0.671996 Loss: 1.187274 | Val Acc: 0.727851 loss: 0.933244\n",
            "[024/100] Train Acc: 0.673014 Loss: 1.180896 | Val Acc: 0.730022 loss: 0.928652\n",
            "saving model with acc 0.730\n",
            "[025/100] Train Acc: 0.675479 Loss: 1.175009 | Val Acc: 0.730063 loss: 0.927176\n",
            "saving model with acc 0.730\n",
            "[026/100] Train Acc: 0.677229 Loss: 1.168362 | Val Acc: 0.730311 loss: 0.924961\n",
            "saving model with acc 0.730\n",
            "[027/100] Train Acc: 0.678667 Loss: 1.162474 | Val Acc: 0.732274 loss: 0.918385\n",
            "saving model with acc 0.732\n",
            "[028/100] Train Acc: 0.679373 Loss: 1.157230 | Val Acc: 0.732067 loss: 0.916120\n",
            "[029/100] Train Acc: 0.681376 Loss: 1.154419 | Val Acc: 0.733067 loss: 0.914286\n",
            "saving model with acc 0.733\n",
            "[030/100] Train Acc: 0.681955 Loss: 1.149034 | Val Acc: 0.733315 loss: 0.916798\n",
            "saving model with acc 0.733\n",
            "[031/100] Train Acc: 0.683612 Loss: 1.141325 | Val Acc: 0.733051 loss: 0.916039\n",
            "[032/100] Train Acc: 0.684191 Loss: 1.138809 | Val Acc: 0.733571 loss: 0.915471\n",
            "saving model with acc 0.734\n",
            "[033/100] Train Acc: 0.685723 Loss: 1.136274 | Val Acc: 0.734348 loss: 0.909044\n",
            "saving model with acc 0.734\n",
            "[034/100] Train Acc: 0.686528 Loss: 1.132496 | Val Acc: 0.735336 loss: 0.907835\n",
            "saving model with acc 0.735\n",
            "[035/100] Train Acc: 0.687428 Loss: 1.127072 | Val Acc: 0.735169 loss: 0.906021\n",
            "[036/100] Train Acc: 0.687967 Loss: 1.124271 | Val Acc: 0.736035 loss: 0.906544\n",
            "saving model with acc 0.736\n",
            "[037/100] Train Acc: 0.689556 Loss: 1.118817 | Val Acc: 0.735714 loss: 0.904733\n",
            "[038/100] Train Acc: 0.689752 Loss: 1.118099 | Val Acc: 0.737035 loss: 0.902584\n",
            "saving model with acc 0.737\n",
            "[039/100] Train Acc: 0.691353 Loss: 1.114412 | Val Acc: 0.737242 loss: 0.899204\n",
            "saving model with acc 0.737\n",
            "[040/100] Train Acc: 0.692153 Loss: 1.110154 | Val Acc: 0.736449 loss: 0.901216\n",
            "[041/100] Train Acc: 0.693230 Loss: 1.104967 | Val Acc: 0.737051 loss: 0.902079\n",
            "[042/100] Train Acc: 0.694532 Loss: 1.101571 | Val Acc: 0.736978 loss: 0.902420\n",
            "[043/100] Train Acc: 0.694923 Loss: 1.098502 | Val Acc: 0.738198 loss: 0.899287\n",
            "saving model with acc 0.738\n",
            "[044/100] Train Acc: 0.695123 Loss: 1.097026 | Val Acc: 0.738921 loss: 0.894928\n",
            "saving model with acc 0.739\n",
            "[045/100] Train Acc: 0.696715 Loss: 1.094808 | Val Acc: 0.738169 loss: 0.896508\n",
            "[046/100] Train Acc: 0.696844 Loss: 1.090752 | Val Acc: 0.739474 loss: 0.894874\n",
            "saving model with acc 0.739\n",
            "[047/100] Train Acc: 0.697560 Loss: 1.088752 | Val Acc: 0.739039 loss: 0.895826\n",
            "[048/100] Train Acc: 0.698498 Loss: 1.085149 | Val Acc: 0.739437 loss: 0.894483\n",
            "[049/100] Train Acc: 0.697813 Loss: 1.085009 | Val Acc: 0.739397 loss: 0.894092\n",
            "[050/100] Train Acc: 0.698767 Loss: 1.084241 | Val Acc: 0.739397 loss: 0.893538\n",
            "[051/100] Train Acc: 0.700331 Loss: 1.077883 | Val Acc: 0.738486 loss: 0.896073\n",
            "[052/100] Train Acc: 0.700862 Loss: 1.077130 | Val Acc: 0.739669 loss: 0.891979\n",
            "saving model with acc 0.740\n",
            "[053/100] Train Acc: 0.701060 Loss: 1.073805 | Val Acc: 0.740059 loss: 0.892287\n",
            "saving model with acc 0.740\n",
            "[054/100] Train Acc: 0.702044 Loss: 1.071000 | Val Acc: 0.739759 loss: 0.891971\n",
            "[055/100] Train Acc: 0.702504 Loss: 1.070022 | Val Acc: 0.741092 loss: 0.891908\n",
            "saving model with acc 0.741\n",
            "[056/100] Train Acc: 0.703023 Loss: 1.067725 | Val Acc: 0.741068 loss: 0.888420\n",
            "[057/100] Train Acc: 0.703931 Loss: 1.064917 | Val Acc: 0.740324 loss: 0.892664\n",
            "[058/100] Train Acc: 0.704502 Loss: 1.062594 | Val Acc: 0.741234 loss: 0.890331\n",
            "saving model with acc 0.741\n",
            "[059/100] Train Acc: 0.705261 Loss: 1.059152 | Val Acc: 0.740084 loss: 0.889572\n",
            "[060/100] Train Acc: 0.705589 Loss: 1.059842 | Val Acc: 0.741263 loss: 0.887579\n",
            "saving model with acc 0.741\n",
            "[061/100] Train Acc: 0.706044 Loss: 1.056726 | Val Acc: 0.741112 loss: 0.888882\n",
            "[062/100] Train Acc: 0.706172 Loss: 1.056434 | Val Acc: 0.741507 loss: 0.887977\n",
            "saving model with acc 0.742\n",
            "[063/100] Train Acc: 0.706811 Loss: 1.053413 | Val Acc: 0.740433 loss: 0.888826\n",
            "[064/100] Train Acc: 0.707259 Loss: 1.052019 | Val Acc: 0.742112 loss: 0.887192\n",
            "saving model with acc 0.742\n",
            "[065/100] Train Acc: 0.707357 Loss: 1.049897 | Val Acc: 0.741783 loss: 0.887508\n",
            "[066/100] Train Acc: 0.707782 Loss: 1.049107 | Val Acc: 0.741864 loss: 0.885727\n",
            "[067/100] Train Acc: 0.708098 Loss: 1.046883 | Val Acc: 0.742596 loss: 0.885621\n",
            "saving model with acc 0.743\n",
            "[068/100] Train Acc: 0.708517 Loss: 1.045485 | Val Acc: 0.742149 loss: 0.888659\n",
            "[069/100] Train Acc: 0.709450 Loss: 1.042923 | Val Acc: 0.742234 loss: 0.885177\n",
            "[070/100] Train Acc: 0.710164 Loss: 1.040969 | Val Acc: 0.742499 loss: 0.885438\n",
            "[071/100] Train Acc: 0.710295 Loss: 1.040390 | Val Acc: 0.741856 loss: 0.884139\n",
            "[072/100] Train Acc: 0.710443 Loss: 1.037971 | Val Acc: 0.741913 loss: 0.886195\n",
            "[073/100] Train Acc: 0.711154 Loss: 1.036185 | Val Acc: 0.742417 loss: 0.886229\n",
            "[074/100] Train Acc: 0.712145 Loss: 1.033199 | Val Acc: 0.742612 loss: 0.888017\n",
            "saving model with acc 0.743\n",
            "[075/100] Train Acc: 0.712067 Loss: 1.034176 | Val Acc: 0.742641 loss: 0.885845\n",
            "saving model with acc 0.743\n",
            "[076/100] Train Acc: 0.712230 Loss: 1.032365 | Val Acc: 0.742413 loss: 0.885058\n",
            "[077/100] Train Acc: 0.712593 Loss: 1.030715 | Val Acc: 0.742860 loss: 0.886802\n",
            "saving model with acc 0.743\n",
            "[078/100] Train Acc: 0.713154 Loss: 1.026449 | Val Acc: 0.743194 loss: 0.884266\n",
            "saving model with acc 0.743\n",
            "[079/100] Train Acc: 0.712649 Loss: 1.028990 | Val Acc: 0.743397 loss: 0.884791\n",
            "saving model with acc 0.743\n",
            "[080/100] Train Acc: 0.713847 Loss: 1.026916 | Val Acc: 0.743678 loss: 0.884089\n",
            "saving model with acc 0.744\n",
            "[081/100] Train Acc: 0.714150 Loss: 1.023759 | Val Acc: 0.742751 loss: 0.884569\n",
            "[082/100] Train Acc: 0.714218 Loss: 1.022235 | Val Acc: 0.742934 loss: 0.881641\n",
            "[083/100] Train Acc: 0.715041 Loss: 1.022733 | Val Acc: 0.743413 loss: 0.884491\n",
            "[084/100] Train Acc: 0.714700 Loss: 1.022225 | Val Acc: 0.743767 loss: 0.883710\n",
            "saving model with acc 0.744\n",
            "[085/100] Train Acc: 0.715937 Loss: 1.018860 | Val Acc: 0.743682 loss: 0.883461\n",
            "[086/100] Train Acc: 0.715432 Loss: 1.019385 | Val Acc: 0.743966 loss: 0.882985\n",
            "saving model with acc 0.744\n",
            "[087/100] Train Acc: 0.715812 Loss: 1.018297 | Val Acc: 0.743397 loss: 0.882630\n",
            "[088/100] Train Acc: 0.716325 Loss: 1.015845 | Val Acc: 0.744304 loss: 0.887685\n",
            "saving model with acc 0.744\n",
            "[089/100] Train Acc: 0.716508 Loss: 1.015356 | Val Acc: 0.744145 loss: 0.885038\n",
            "[090/100] Train Acc: 0.717183 Loss: 1.013931 | Val Acc: 0.744141 loss: 0.884729\n",
            "[091/100] Train Acc: 0.717871 Loss: 1.010739 | Val Acc: 0.744824 loss: 0.883863\n",
            "saving model with acc 0.745\n",
            "[092/100] Train Acc: 0.717444 Loss: 1.011270 | Val Acc: 0.744523 loss: 0.883607\n",
            "[093/100] Train Acc: 0.718571 Loss: 1.010901 | Val Acc: 0.744954 loss: 0.884706\n",
            "saving model with acc 0.745\n",
            "[094/100] Train Acc: 0.718470 Loss: 1.008224 | Val Acc: 0.744434 loss: 0.881425\n",
            "[095/100] Train Acc: 0.718452 Loss: 1.007987 | Val Acc: 0.743836 loss: 0.886059\n",
            "[096/100] Train Acc: 0.718853 Loss: 1.006281 | Val Acc: 0.743970 loss: 0.883738\n",
            "[097/100] Train Acc: 0.718764 Loss: 1.007426 | Val Acc: 0.744804 loss: 0.880765\n",
            "[098/100] Train Acc: 0.719348 Loss: 1.004266 | Val Acc: 0.744657 loss: 0.884844\n",
            "[099/100] Train Acc: 0.719017 Loss: 1.005208 | Val Acc: 0.744678 loss: 0.883116\n",
            "[100/100] Train Acc: 0.720079 Loss: 1.002592 | Val Acc: 0.745641 loss: 0.881533\n",
            "saving model with acc 0.746\n"
          ]
        }
      ],
      "source": [
        "# start training\n",
        "\n",
        "best_acc = 0.0\n",
        "for epoch in range(num_epoch):\n",
        "    train_acc = 0.0\n",
        "    train_loss = 0.0\n",
        "    val_acc = 0.0\n",
        "    val_loss = 0.0\n",
        "\n",
        "    # training\n",
        "    model.train() # set the model to training mode\n",
        "    for i, data in enumerate(train_loader):  # 对于每一个批次\n",
        "        inputs, labels = data  # 获取本次数据和标签\n",
        "        inputs, labels = inputs.to(device), labels.to(device)  # 根据前面获取的device设置，放入GPU或CPU\n",
        "        optimizer.zero_grad()  # 初始梯度清零，防止梯度累积\n",
        "        outputs = model(inputs)  # 前向传播，获得结果outputs\n",
        "        batch_loss = criterion(outputs, labels)  # 计算当前批次的损失\n",
        "        _, train_pred = torch.max(outputs, 1) # get the index of the class with the highest probability，其实就是获取预测的类别\n",
        "        batch_loss.backward()  # 反向传播计算梯度\n",
        "        optimizer.step()  # 更新学习率\n",
        "\n",
        "        # 计算训练时的准确率和损失\n",
        "        # 准确率的计算方式是：累加预测的类别和实际标签相同的个数\n",
        "        train_acc += (train_pred.cpu() == labels.cpu()).sum().item()\n",
        "        train_loss += batch_loss.item()\n",
        "\n",
        "    # validation\n",
        "    if len(val_set) > 0:\n",
        "        model.eval() # set the model to evaluation mode\n",
        "        with torch.no_grad():  # 禁止梯度计算，以节省内存和计算时间\n",
        "            for i, data in enumerate(val_loader):  # 同样对于每个批次，大体和训练时相同，只是不需要计算梯度和更新参数\n",
        "                inputs, labels = data\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                batch_loss = criterion(outputs, labels) \n",
        "                _, val_pred = torch.max(outputs, 1) \n",
        "            \n",
        "                val_acc += (val_pred.cpu() == labels.cpu()).sum().item() # get the index of the class with the highest probability\n",
        "                val_loss += batch_loss.item()\n",
        "\n",
        "            # 打印出结果以查看训练情况，实际上前面计算的都是预测正确的数量，这里打印准确率采用直接除法得出结果\n",
        "            print('[{:03d}/{:03d}] Train Acc: {:3.6f} Loss: {:3.6f} | Val Acc: {:3.6f} loss: {:3.6f}'.format(\n",
        "                epoch + 1, num_epoch, train_acc/len(train_set), train_loss/len(train_loader), val_acc/len(val_set), val_loss/len(val_loader)\n",
        "            ))\n",
        "\n",
        "            # if the model improves, save a checkpoint at this epoch\n",
        "            if val_acc > best_acc:  # 只有模型性能更好时保存参数\n",
        "                best_acc = val_acc\n",
        "                torch.save(model.state_dict(), model_path)\n",
        "                print('saving model with acc {:.3f}'.format(best_acc/len(val_set)))\n",
        "    else:\n",
        "        # 如果没有验证集，则直接输出训练集结果\n",
        "        print('[{:03d}/{:03d}] Train Acc: {:3.6f} Loss: {:3.6f}'.format(\n",
        "            epoch + 1, num_epoch, train_acc/len(train_set), train_loss/len(train_loader)\n",
        "        ))\n",
        "\n",
        "# if not validating, save the last epoch\n",
        "if len(val_set) == 0:\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "    print('saving model at last epoch')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Hi7jTn3PX-m"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfUECMFCn5VG"
      },
      "source": [
        "Create a testing dataset, and load model from the saved checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PKjtAScPWtr",
        "outputId": "8c17272b-536a-4692-a95f-a3292766c698"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\BQ\\AppData\\Local\\Temp\\ipykernel_14476\\2373171232.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path))\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# create testing dataset\n",
        "test_set = TIMITDataset(test, None)\n",
        "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# create model and load weights from checkpoint\n",
        "model = Classifier().to(device)\n",
        "model.load_state_dict(torch.load(model_path))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "940TtCCdoYd0"
      },
      "source": [
        "Make prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "84HU5GGjPqR0"
      },
      "outputs": [],
      "source": [
        "predict = []\n",
        "model.eval() # set the model to evaluation mode\n",
        "with torch.no_grad():\n",
        "    for i, data in enumerate(test_loader):\n",
        "        inputs = data\n",
        "        inputs = inputs.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, test_pred = torch.max(outputs, 1) # get the index of the class with the highest probability\n",
        "\n",
        "        for y in test_pred.cpu().numpy():\n",
        "            predict.append(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWDf_C-omElb"
      },
      "source": [
        "Write prediction to a CSV file.\n",
        "\n",
        "After finish running this block, download the file `prediction.csv` from the files section on the left-hand side and submit it to Kaggle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "GuljYSPHcZir"
      },
      "outputs": [],
      "source": [
        "with open('prediction.csv', 'w') as f:\n",
        "    f.write('Id,Class\\n')\n",
        "    for i, y in enumerate(predict):\n",
        "        f.write('{},{}\\n'.format(i, y))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "SHARE MLSpring2021 - HW2-1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
